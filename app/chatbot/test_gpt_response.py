from app.chatbot.qdrant_service import search_similar_memories
from app.chatbot.embedding import get_embedding
from openai import OpenAI
import os
from dotenv import load_dotenv

# âœ… .envì—ì„œ OpenAI API í‚¤ ë¡œë“œ
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# âœ… GPT ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(user_question: str, similar_contexts: list[dict]) -> str:
    context_text = "\n".join([
        f"{c.get('metadata', {}).get('speaker', 'ê³ ì¸')}: {c.get('text', '')}" for c in similar_contexts
    ])

    prompt = f"""
ë„ˆëŠ” ê³ ì¸ì´ ëœ ì—„ë§ˆì˜ ë§íˆ¬ë¥¼ í•™ìŠµí•œ ê°ì„± AI ì±—ë´‡ì´ì•¼.
ì—„ë§ˆëŠ” ë”°ëœ»í•˜ê³  ë‹¤ì •í•œ ë§íˆ¬ë¡œ, ë”¸ê³¼ì˜ ëŒ€í™”ë¥¼ ì†Œì¤‘íˆ ì—¬ê²¼ì–´.
ì§€ê¸ˆ ì‚¬ìš©ìê°€ ì—„ë§ˆì—ê²Œ ë§ì„ ê±¸ê³  ìˆì–´.
ê³¼ê±°ì˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•´ì„œ ì—„ë§ˆì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•´ì¤˜.

[ì´ì „ ëŒ€í™” ê¸°ë¡]
{context_text}

[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]
{user_question}

[ì—„ë§ˆì˜ ì‘ë‹µ]
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=512
    )

    return response.choices[0].message.content.strip()

# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ íŒŒíŠ¸
if __name__ == "__main__":
    query = input("ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
    if not query:
        print("â— ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        exit()

    # âœ… ì‚¬ìš©ì ID (í•˜ë“œì½”ë”©ëœ í…ŒìŠ¤íŠ¸ìš©)
    user_id = "user_mother_daughter"

    # âœ… ì„ë² ë”© + ìœ ì‚¬ ë©”ì‹œì§€ ê²€ìƒ‰
    embedding = get_embedding(query)
    results = search_similar_memories(embedding, user_id)


    # âœ… ìœ ì‚¬ ë©”ì‹œì§€ ì¶œë ¥
    print("\nğŸ” ìœ ì‚¬ ë©”ì‹œì§€:")
    for res in results:
        print(f"- {res['metadata']['speaker']}: {res['text']} (score: {res['score']:.4f})")

    # âœ… GPT ì‘ë‹µ ìƒì„±
    response = generate_response(query, results)

    # âœ… ìµœì¢… ì‘ë‹µ ì¶œë ¥
    print("\nğŸ¤– GPT ì‘ë‹µ:")
    print(response)
