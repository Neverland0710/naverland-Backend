# âœ… gpt_service.py (LangChain + Runnable ê¸°ë°˜ìœ¼ë¡œ ì „í™˜)

import os
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory

from chatbot.embedding import get_embedding
from chatbot.qdrant_service import search_similar_memories

load_dotenv()

# âœ… GPT ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(
    model_name="gpt-4o",
    temperature=0,
    openai_api_key=os.getenv("OPENAI_API_KEY")
)


# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë„ˆëŠ” ê°ì„± AI ì¶”ëª¨ ì±—ë´‡ì´ì•¼. ê³ ì¸ì˜ ë§íˆ¬ë¥¼ ì¬í˜„í•´ì„œ ìœ ì¡±ì—ê²Œ ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•´ì¤˜."),   
    ("placeholder", "{chat_history}"),
    ("user", "{input}")
])

# âœ… Runnable Chain + ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
chat_chain = RunnableWithMessageHistory(
    prompt | llm,
    lambda session_id: InMemoryChatMessageHistory(),
    input_messages_key="input",
    history_messages_key="chat_history"
).with_config({"run_name": "chat-runnable"})

# âœ… ì‘ë‹µ ìƒì„± í•¨ìˆ˜

def generate_response(user_id: str, user_question: str) -> str:
    try:
        # 1. ì„ë² ë”©
        query_vector = get_embedding(user_question)
        if not query_vector:
            return "âš ï¸ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë° ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        # 2. ìœ ì‚¬ ë¬¸ë§¥ ê²€ìƒ‰
        similar_contexts = search_similar_memories(query_vector, user_id)

        # 3. ë¬¸ë§¥ ë¶™ì´ê¸°
        context_lines = [
            f"{c['metadata'].get('speaker', 'ê³ ì¸')}: {c['text']}" for c in similar_contexts
        ]
        context = "\n".join(context_lines)
        combined_input = f"{context}\n\n{user_question}" if context else user_question

        # 4. RunnableWithMessageHistory ì‹¤í–‰
        result = chat_chain.invoke(
            {"input": combined_input},
            config={"configurable": {"session_id": user_id}}
        )

        # 5. ì‘ë‹µ ë°˜í™˜
        return result["output"].strip()

    except Exception as e:
        import traceback
        print("âŒ GPT í˜¸ì¶œ ì‹¤íŒ¨:", e)
        traceback.print_exc()
        return "âš ï¸ ëŒ€ë‹µì„ ì¤€ë¹„í•˜ì§€ ëª»í–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

# âœ… ì˜ˆì‹œ ì‹¤í–‰ìš© (ê°œë°œ ì¤‘ í…ŒìŠ¤íŠ¸)
if __name__ == "__main__":
    answer = generate_response(
        user_id="user_mother_daughter",
        user_question="ê°•ë¦‰ ê°”ë˜ê±° ê¸°ì–µë‚˜?"
    )
    print("ğŸ’¬ ì‘ë‹µ:", answer)
